[
  {
    "objectID": "posts/jmir2015.html",
    "href": "posts/jmir2015.html",
    "title": "Publishing Bayesian analyses in a medical journal",
    "section": "",
    "text": "Publishing Bayesian analyses in a medical journal\n\n\n\n\n\nI am excited to announce that our paper Predictors of response to Web-based cognitive behavioral therapy with face-to-face therapist support for depression: A Bayesian analysis has been accepted for publication in the Journal of Medical Internet Research (see below for the full citation). This paper is the result of a fruitful collaboration with Martin Eisemann's research group for Mental e-Health at the University of Tromsø.\nThe project (that was led by soon-to-be-doctor Ragnhild Høifødt) attempted to find individual-level variables that could help to predict whether a web-based treatment for depression would be effective or not. While the actual finding in the paper are fascinating (and I do recommend that you read it!), here I would like to say a few words about my experiences about publishing the Bayesian analyses (that are promised in the title). This is my second paper that used entirely Bayesian statistics (my first one was a paper on the neural basis of mind-wandering) and I learned a lot while working on the paper, both about appropriate analyses for this kind of data and about the process of publishing such analyses in communitites that are not yet used to it. The technical details are pretty much spelled out in the supplemental material (and, if you ask kindly, I am more than happy to share my R/JAGS code).\nOne issue that was immediately obvious to me was the need for a proper documentation of the used models. It is all to common for papers in area to say something like \"linear mixed growth-models were used\" relying on the common language of the readership and assuming that everyone who will find this paper interesting in the future will immediately be able to infer exactly which analysis was run. I think this is a misconception. Even standard models have different names in different literatures and there are many different ways to calculate p-values, confidence intervals etc for more complicated statistical analyses. Also, standard methods can and will change in the future and it is not guaranteed that a researcher in the field twenty years in the future will be able to infer (and reproduce!) the exact analyses that were conducted. While people get away with this sort of thing when using null-hypothesis-testing (presumably because at least the reviewers DO know which models have been run as there are limited options), a Bayesian model is individually tailored to its application and needs a careful description. I therefore wrote up the model in a very unusual format, detailing all used distributionary assumptions, parameter transformations, priors etc. The methods portion grew accordingly and it was pretty obvious that this would be a hard-to-read paper. Still, I did what I felt was the right thing and sent it out to JMIR for review.\nI was positively surprised by the generally positive attitude of the editor and the reviewers who welcomed the Bayesian approach. However, one of the reviewers mentioned that\n\n[…] the justification of Bayesian methods seem somewhat over-enthuasiastic and perhaps unnecessarily dismissive of null-hypthesis testing (I am no fan of null-hypothesis testing either, but still had some doubts with regard to the way this was justified here). Bayesian methods are introduced as being \"preferable\", \"more flexible\", \"more readily interpretable\"; and so forth - but they also strike me (at least the way they are presented here) as complex to the point of seeming obsure (at least to the non-expert).\n\nFair enough -- it took me 6 pages to detail the used models while most methods section in similar papers using traditional models use maybe a paragraph. Here is my response:\n\n[…] the reviewer argues that Bayesian methods are perceived as being too complex or even obscure. We argue, that this perception is solely due to the conventional practice in publishing statistical analysis. A standardized frequentist analysis (e.g., a linear mixed model fit by ML) is easy to follow for the educated reader but this is solely because of the highly standardized nature of this type of analysis. A reader from a different field of study (where statistical nomenclature and standards may be different) will have a hard time reading a \"standard\" statistical analysis that are so familiar to us. We wrote the methods section in a way that it is comprehensible for a statistically educated reader from any field but we do acknowledge that this came at the cost of readability for more applied readers. If standard analyses would be reported in the same comprehensive way (discussing all hidden assumptions), the description would most certainly fill more pages than we used and seem quite obscure (given that many of the assumptions and concepts underlying NHST are much less intuitive than the Bayesian equivalents).\n\nThis is basically what I wrote above, if you want to write up all the details of traditional methods, the section would be just as long. Anyway, the reviewer went on to make some important points. In particular I found this statement interesting:\n\nMoreover, a main argument for the Bayesian methods seems to be that they present a welcome shift away from arbitrary accepting or rejecting hypotheses based on whether the significance level of .05 is achieved or not. If this is the case, then why present the \"Bayes Factor\" statistics, which seem to again introduce somewhat rigid guidelines as to when H1 or H0 is supported (e.g., Bayes Factor > 10: \"strong evidence for H1\"). Wouldn't a more conventional presentation of effect sizes and confidence intervals achieve the same purpose, but in a much more efficient manner (and one that might be more familiar to most readers, although \"familiarity is better\" is admittedly a very weak argument)?\n\nIt refers to Jeffrey's table that assigns labels to different ranges the Bayes factor can take (it is on wikipedia) and I agree that these labels should be taken with a grain of salt. Here is what I responded:\n\nBayes Factors quantify the degree of evidence that the data provides in favor of either the null- or the alternative hypothesis. It is thus fundamentally different from p-values (or confidence intervals, at that). However, the interpretation provided in Table 1 to which the reviewer objects is indeed not uncontroversial (similar tables with different wordings and different cut-off values have been proposed). We do not want to put too much emphasis at these labels but rather focus on the exact interpretation of the Bayes Factor: how much more likely one hypothesis is than the other.\n\nI was, however, a little afronted by the suggestion that effect sizes and confidence intervals would carry the same information in a more efficient way:\n\nThe information contained in the BF is not identical to effect sizes and/or confidence intervals and we argue strongly that confidence intervals are neither more efficient nor more easily interpreted. In fact, research shows that confidence intervals are robustly misinterpreted even by trained researchers (Hoekstra et al., 2014). In their study, they asked a sample of psychological researchers conceptual questions about which questions confidence intervals could or could not answer and they found that 97 % of the researchers failed to correctly answer all questions (meaning that 97% of the researchers fell into at least one fallacy surrounding the interpretation of confidence intervals). We are of the opinion that Bayes Factors are far easier interpretable, by comparison. As an alternative to effect sizes + confidence intervals, we do report the mode of the posterior distribution of the parameter estimates and their 95% highest density interval (the Bayesian equivalent of a confidence interval that is much less subject to misinterpretation) […].\n\nAlso, in the meantime, I took the time to read Morey's excellent paper The Fallacy of Placing Confidence in Confidence Intervals wich contains some excellent and pedagogical examples how confidence intervals can be misleading.\nAfter long discussions, we concluded in outsourcing the technical details of the Bayesian analysis into a supplemental material and wrote a short, relatively superficial summary for the main paper. I think this is a rather nice way to trade off readability and comprehensibility and think that I will use this model in the future. However, it has several shortcomings:\n\nthe supplemental material is not itself citable\nwhile the supplemental material does undergo peer review, it is probably not subject to the same strict quality checks as the main paper (I often find typos and more serious errors in these materials).\n\nGiven that these supplemental materials are often the most important source of information when considering reproducibility of the research (especially in papers using the ultra-short format that is typical for high-impact journals), I think that their status should be enhanced. One possibility would be to publish two versions of the same paper, one \"long\" version that includes all the details and a \"short\" version that does not. In todays world where no one actually prints full journals anymore (or does that still happen?), this should not be too much of a problem. A second method would be to make supplemental little stand-alone papers that get their own DOI such that they are at least directly citable.\n\nReferences\n\nHøifødt RS, Mittner M, Lillevoll K, Katla SK, Kolstrup N, Eisemann M, Friborg O, Waterloo K Predictors of Response to Web-Based Cognitive Behavioral Therapy With High-Intensity Face-to-Face Therapist Guidance for Depression: A Bayesian Analysis J Med Internet Res 2015;17(9):e197 URL: http://www.jmir.org/2015/9/e197 DOI: 10.2196/jmir.4351 PMID: 26333818\nMittner, M., Boekel, W., Tucker, A. M., Turner, B.M., Heathcote, A. and Forstmann, B.U. (2014). When the brain takes a break: A model-based analysis of mind wandering. Journal of Neuroscience, 34(49):16286-95."
  },
  {
    "objectID": "posts/latexdiff.html",
    "href": "posts/latexdiff.html",
    "title": "Highlight changes with latexdiff",
    "section": "",
    "text": "Highlight changes with latexdiff\nHave you ever been asked to provide a resubmission with \"changes highlighted\"? Or have you ever received a modified version of your latex file and wanted to see the changes in a PDF version?\nWell I have and here's my solution.\nI use to track my papers with git which makes the whole process of integrating your co-authors opinions a breeze.\nI will just guide you through the process beginning at zero:\n\ndo the research\nwhen ready to write the paper, make a new directory and initialize it with git\n\nmkdir awesome_paper\ncd awesome_paper\ngit init\n\ncreate your paper, images and whatnot. Log changes into git with\n\ngit add paper.tex\ngit add pics/*.pdf\ngit commit -a -m \"your commit message goes here\"\n\nsend your version to your co-authors and receive feedback paper_with_some_changes.tex\nuse latexdiff to create a nice highlighted PDF\n\nlatex-diff paper.tex paper_with_some_changes.tex > paper_diff.tex\npdflatex paper_diff.tex\n5. submit your paper to your journal of choice (e.g., the Journal in Support of the Null-Hypothesis)\n\nreceive angry feedback from the reviewers\nbefore hacking your paper in reply to the undoubtedly ill-informed and biased crangling of the reviewers, tag the paper to make sure you find the version you submitted\n\ngit tag first_submission\n\nhack your paper and shut up those ignorant reviewers\nbeing forced to resubmit a \"changes highlighted\" version of your paper (driving home the point how much you had to mutilize your beautiful paper), do the following:\n\ngit show first_submission:paper.tex > paper_firstsub.tex \nlatexdiff paper_firstsub.tex paper.tex > paper_diff.tex\npdflatex paper_diff.tex\nbibtex paper_diff\npdflatex paper_diff.tex\n\nsubmit and keep your fingers crossed!\n\nI like to put the last block of commands into a Makefile such that all you have to do is a\nmake first_diff  # results in highlighted version\nmake             # results in clutter-free paper\nHere is the Makefile:\nTARG=paper\n\n$TARG.pdf: $TARG.tex myrefs.bib\n     pdflatex $TARG.tex\n     bibtex $TARG.tex\n     pdflatex $TARG.tex\n\nfirst_diff: \n     git show first_submission:$TARG.tex > $TARG_firstsub.tex \n     latexdiff $TARG_firstsub.tex $TARG.tex > $TARG_diff.tex\n     pdflatex $TARG_diff.tex\n     bibtex $TARG_diff\n     pdflatex $TARG_diff.tex\nYou might want to look at latexdiff's -t switch. You can change how deletions and additions are displayed in the final PDF."
  },
  {
    "objectID": "posts/jupyter.html",
    "href": "posts/jupyter.html",
    "title": "Multi-Language IPython (Jupyter) setup",
    "section": "",
    "text": "The IPython notebook project (which I adore) is about to be renamed to Jupyter to better reflect the fact that it can be used with other languages besides Python. At the moment that is under heavy development and probably quite unstable but I found myself aching for an IR-Notebook. So here is a guide to set up a multi-language Jupyter notebook (meaning that you just start the server and can select if you want an R-Notebook, a Python-notebook, or one of the other availabel kernels).\nNote that the instructions on this page are likely to break since they depend on early development versions of the different packages. I expect that the whole procedure is going to be faciliated/documented once the Jupyter project gets going.\nAt first, we set up a bleeding-edge ipython (this is from a bare bones python installation):\n# setup pip\nwget https://bootstrap.pypa.io/get-pip.py  \nsudo python get-pip.py\n\n# make a virtualenv\nsudo pip install virtualenv\ncd ~\nvirtualenv local/python/jupyter\nsource local/python/jupyter/bin/activate\n\n## get the cutting edge ipython version\ngit clone --recursive https://github.com/ipython/ipython.git\n# and install all its dependencies\ncd ipython\npip install -e \".[notebook]\" \nYou can run the notebook with\nipython notebook\nwhile you are in the virtualenv. Stay in the virtualenv for the rest of the setup.\n\n\nFor adding R-support, you need the experimental IRKernel. Just follow the instructions in the `README`:\nInstall dependencies:\nsudo apt-get install libzmq3-dev libcurl4-openssl-dev\nfire up R and\ninstall.packages(\"devtools\")\n# Need RCurl for install_github\ninstall.packages('RCurl')\nlibrary(devtools)\ninstall_github('armstrtw/rzmq')\ninstall_github(\"takluyver/IRdisplay\")\ninstall_github(\"takluyver/IRkernel\")\nIRkernel::installspec() \nIn my case the last line failed with permission problems. The code of IRkernel::installspec is as follows:\nfunction () \n{\n     srcdir = system.file(\"kernelspec\", package = \"IRkernel\")\n     cmd = paste(\"ipython kernelspec install --replace --name ir\", \n           srcdir, sep = \" \")\n     system(cmd, wait = TRUE)\n}\nso the function basically makes a system call to ipython kernelspec to install the kernel that is located in srcdir. You can find out the value of srcdir by running the first line of the function\nprint(system.file(\"kernelspec\", package = \"IRkernel\"))\n(in my case the path was /home/mittner/R/x86_64-unknown-linux-gnu-library/3.1/IRkernel/kernelspec).\nNow exit R and run the command by hand (supplemented by \"--user\" to install the kernel in the users directory instead of globally -- that was what caused the permission problem):\nipython kernelspec install --replace --name ir --user /home/mittner/R/x86_64-unknown-linux-gnu-library/3.1/IRkernel/kernelspec\nNow fire up the notebook and voila:\n\nand by creating an R-notebook, you have the RNotebook:\n\nFinally, it's possible to add more kernels for example for Julia in a similar way."
  },
  {
    "objectID": "posts/clavinova.html",
    "href": "posts/clavinova.html",
    "title": "(Dis-)Assembling of Clavinova CLP-970",
    "section": "",
    "text": "(Dis-)Assembling a Clavinova CLP-970\n\nI recently had to disassemble my clavinova CLP-970 for various reasons.\nI followed the excellently written guide from this blog post (by Sylvia) and integrated this text with images. I didn’t really have anything to add to this comprehensive text so in the following I just copied her text:\nI was able to perform the entire task using a single long (15cm shaft) cross-headed screw-driver that I had magnetised, and a thin piece of metal, which in my case was a paint scraper.\nIn the discussion, “backwards” means towards the back of the piano. Related words take their meanings accordingly.\n\nMake sure the piano is unplugged from the mains.\nThe music stand is attached using keyhole mountings. Remove it by sliding it upwards by a couple of centimeters and it will unlock and can be easily be pulled forwards. Be careful that it doesn’t simply fall off!\n\n\n\nRemove the two large brass mountings that held the music stand.\n\n\n\nThe one centimeter thick frame for the fabric speaker cover is now held only by Velcro, and can be pulled off from either side.\n\n\n\nAt each end of the speaker enclosure there is a large slot through which a screw can be reached. Remove the two screws (that is, one from each end).\n\n\n\nThe speaker enclosure is hinged at the back, and can now be tilted right over until it is completed inverted. Make sure there is a clear space in which to do this, and note that the enclosure is heavy.\n\n\n\n\nI completely removed the keyboard cover. I’m not sure now that that’s necessary. It may be possible to deal with keyboard without actually removing the cover.\n\n\nWith the keyboard cover closed, locate the two plastic clips that cover an opening in the keyboard track slot. Remove the two screws (note, they are very small, and accordingly easy to drop and lose), and remove the plastic clips.\n\n\n\n\nOpen the keyboard cover until it is possible to lift the keyboard axle (which has a gear wheel at each end) out of the slot.\nWith the axle now running on top of its slot, position the keyboard cover so that it is about half closed, then lift the rear until the cover is vertical. Slowly turn your body until the keyboard cover front runners disengage. The keyboard cover is now free. You shouldn’t have to apply any force.\n\n\n\nThe control panel is attached to the base by 5 metal brackets, each containing two screws. Remove the screws. There is also another screw at the left hand side, which also needs to be removed. NOTE this is not the same type of screw as the other ten. It’s a self-tapper with a blunt end.\n\n\n\n\nRemove the screw that attaches the black grounding wire to the metallic base.\n\n\n\nThe control panel can now be lifted up (CAREFULLY!) and rested on the keyboard cover track. It is still attached to the base by wires, which must not be stretched. The reason the black grounding wire was detached in step 8 is that it is not long enough to allow this step without its being disconnected.\n\n\n\nRemove the nine large and two small brass screws that hold the keyboard in place.\n\n\n\nRemove the wooden blocks. Each block is held in place by a single screw, but has prongs at the front that engage with the case, so it has to be moved backwards a bit before it is completely free. The right hand one is in any case still attached to wires, and can only be moved out of the way.\n\n\n\nLift up the front of the keyboard, and detach the multi-wire connector from its socket at the front middle of the keyboard. It should just pull out, but be gentle. Do not pull by the wires! You may also see that the wires are attached to the keyboard by some adhesive tape, which will have to be removed.\n\n\n\n\nOnly if you want to completely remove the keyboard: At the back of the keyboard gently prise open the plastic clip that holds the wires in place at the back of the keyboard, and release the wires.\n\n\nIn the original guide, there is more information on removing single keys. I didn’t do that here.\nReassembly - the reverse of the above.\n\nWhen plugging the connector back into the keyboard, note that it will only go in one way round. Again, be gentle. It shouldn’t require much force.\nWhen replacing the right hand block of wood containing the power switch, you will need to watch where the wires go. Both blocks have metal prongs at the front, which need to engage into holes in the frame.\nWhen putting the keyboard cover back, and before trying to close it, make sure that both ends of the axle can reach their rear end stops. If only one can, then you’ll have to lift one end of the axle out of the slot, and rotate it a bit. Repeat until correct.\nThe keyboard track clips are mirror images of each other, and are marked L and R accordingly.\nWhen replacing the fabric speaker cover frame, be careful not to push where the fabric covers the speakers. Note that there are central plastic locator pins that have to engage with holes in the speaker enclosure."
  },
  {
    "objectID": "posts/kincog.html",
    "href": "posts/kincog.html",
    "title": "Association Between Executive Functions, Working Memory, and Manual Dexterity",
    "section": "",
    "text": "New paper (open data!): Association Between Executive Functions, Working Memory, and Manual Dexterity\n\n\n\n\n\nI am thrilled to see that our fully-Bayesian, open-data paper \"Association Between Executive Functions, Working Memory, and Manual Dexterity in Young and Healthy Older Adults\" has been published in Perceptual & Motor Skills. This paper takes on the rarely asked question about the relationship between cognitive functions and manual dexterity. The history of my involvement with this manuscript is maybe unusual in that one of the reviewers actually requested the use of Bayesian statistics (because he felt that the analyses reported in an initial version of the MS in which I was not involved were not appropriate). I was, of course, all too happy to help and our efforts have resulted in a fully-Bayesian paper.\nI have recently started to think about openness of data and materials to the scientific community. Ever since it's initiation, I have been a fan of Richard Morey's Peer Reviewers' Openness Initiative and I have recently signed the initiative (you should do so, too!). At the time of this writing, there are already 278 people who have committed to require that data and experimental materials are made openly available before reviewing a paper. Obviously, making such a commitment requires that I also follow these rules in my own research! So far, I have not done so (shame on me!), mostly for practical reasons (publishing large fMRI data is difficult; anonymization not trivial; documentation of the datasets time-consuming etc, etc) even though I was always committed to give out data/materials when requested (we signed a declaration in my old lab). However, I do see the difference between agreeing to handing out data and putting it up on the web for anyone to look at. How often did I not wish to have easy access to the data published in scientific articles (for research, teaching and sometimes just for fun)! I have therefore decided to implement an open-data policy and upload all data per-default upon publication.\nGiven that usually many co-authors are involved in most research projects, it is not always easy to convince everyone to go along with these requirements. I am happy that my co-authors on our new paper Claudia Rodriguez-Aranda and Olena Vasylenko from the Research group \"Behavioral neuroscience and human development\" readily agreed to share their hard-won data with anyone who can use a computer to download files. The data and all analysis scripts (R + Stan) are available from github.\n\nReferences\n\nRodriguez-Aranda, C., Mittner, M. & Vasylenko, O. (2016). Association between Executive Functions, Working Memory, and Manual Dexterity in Young and Healthy Older Adults: An exploratory study. Perceptual & Motor Skills, 122 (1), 165-192."
  },
  {
    "objectID": "posts/waic_stan.html",
    "href": "posts/waic_stan.html",
    "title": "Fixing numerical underflow in WAIC calculation in Stan",
    "section": "",
    "text": "Fixing numerical underflow in WAIC calculation in Stan\nI recently switched to the amazing software Stan for my MCMC needs. Every now and then, it can be useful to compare different models using some kind of information criterion. Previously, I used the Deviance Information criterion (DIC; Spiegelhalter, 2002) because that appeared to be the most correct for hierarchical Bayesian models. However, DIC received quite some critique on, e.g., Andrew Gelman's blog (here, or here) while the recent addition to the \"XIC\" family WAIC (Watanabe, 2010) appears to have more desirable theoretical properties (see here or there). This paper offers an in-depth discussion on the theory behind these measures.\nRecently, Aki Vehtari and Andrew Gelman showed how to implement the WAIC in Stan and, of course, I had to try it for myself. In my very first model, I stumbled across a problem. When running waic(fit) on my stanfit object, I got\n$total\nwaic       lpd    p_waic elpd_waic     p_loo  elpd_loo \nInf      -Inf  113.7925      -Inf       NaN      -Inf \n\n$se\n[1]      NaN      NaN 20.19901      NaN      NaN      NaN\nHm, not very informative. When I took a look at the waic() function in more detail, I saw that the Inf’s in my output were caused by numerical underflow in the calculation of lpd:\nlpd <- log(colMeans(exp(log_lik)))\nThe exp function can (of course) only handle arguments of a certain magnitude before returning 0. Try, for example,\nexp(seq(0,-1000, by=-10))\nOn my machine, exp bails out at -750 and apparently one of my log-likelihoods went below that magical threshold.\nTo fix this problem, I used the log-sum-exp trick (see e.g., this webpage). This trick improves the numerical stability of calculating the logarithm of the sum of exponentials by making use of the fact that \\[\\log\\left(\n\\sum_{i=1}^{n} \\exp( x_i )\\right)=\\log\\left(\n\\sum_{i=1}^{n} \\exp( x_i-a )\\right)+a\\] and by choosing a suitable number for \\(a\\), e.g., the most extreme value among the \\(x_i\\), the range of values to be calculated within the exp can be reduced. In this case, the mean is calculated for each column and therefore the formula is \\[\\log\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\exp( x_i )\\right)=\\log\\frac{1}{n}+\\log\\left( \\sum_{i=1}^{n} \\exp(\nx_i-a )\\right)+a.\\]\nTo do it for each column in the matrix requires a bit of indexing fun:\n# lpd <- log(colMeans(exp(log_lik)))\noffset <- log_lik[cbind(max.col(abs(t(log_lik))), 1:n)] # column-wise most extreme value\nlpd <- log(1./S)+log(colSums(exp(sweep(log_lik, 2, offset))))+offset\nThis fixes this issue and allows to calculate the WAIC for situations where the log-likelihood is very low. I also compared this solution to a solution where I used an arbitrary-precision library, Rmpfr, to do the calculation on the original data:\n# lpd <- log(colMeans(exp(log_lik)))\nrequire(Rmpfr)\nlog_likp <- mpfr(log_lik, 120) # 120 bits precision\n lpd <- as.numeric(log(colMeans(exp(log_likp)))) # run in arbitrary precision\nand the result is identical to my log-sum-exp solution.\nSee this gist for the complete code (original, log-sum-exp and arbitrary-precision based versions)."
  },
  {
    "objectID": "posts/ipython_ssh.html",
    "href": "posts/ipython_ssh.html",
    "title": "Access IPython Notebook through SSH to avoid firewall",
    "section": "",
    "text": "I usually run ipython notebook on a server and access the web-interface from a different computer. This enables me to start a lengthy computation, shut my laptop and come back once it's done.\nHowever, I recently had the problem that I was not able to find a good way of opening the ipython-specific port in the firewall. Also, it's a bit dangerous to do so.\nI was therefore happy to have stumbled across a blog post which told me how to access ipython through a SSH-tunnel. Basically, this allows to access ipython running on any computer that you can access via SSH.\nHere's the recipe:\n\nOn the server, start the ipython notebook:\n\nipython notebook --no-browser --port=8889\n\nOn the local machine, setup a pipe with\n\nssh -N -n -L localhost:8889:localhost:8889 user@servername\n(this will have to be redone after opening your laptop again)\n\naccess your ipython notebook locally with localhost:8889\n\n\n\nHere is an outline how I setup my IPython to serve all my scientific needs:\n\nmake and activate an virtualenv:\n\nvirtualenv local/python/name\nsource local/python/name/bin/activate\n\nget the most recent ipython\n\n## get the cutting edge ipython version\ngit clone --recursive https://github.com/ipython/ipython.git\n# and install all its dependencies\ncd ipython\npip install -e \".[notebook]\" \n\nget all the cool packages\n\npip install numpy\npip install scipy\npip install matplotlib\npip install pandas\npip install rpy2\npip install seaborn\npip install pymc\npip install statsmodels\npip install ipycache\npip install pushbullet.py"
  },
  {
    "objectID": "posts/orgmode.html",
    "href": "posts/orgmode.html",
    "title": "Setup for sharing org-mode agenda with google calendar",
    "section": "",
    "text": "Note: this is only one-directional from org-mode to google calendar.\n\n\nPut this into the .emacs file (I save the agenda to dropbox).\n(setq org-icalendar-combined-agenda-file \"~/Dropbox/orgfiles/org.ics\")\n(setq org-icalendar-include-todo '(all))\n(setq org-icalendar-use-scheduled '(event-if-todo event-if-not-todo)) \n(setq org-icalendar-use-deadline '(event-if-todo event-if-not-todo))\n\n;; this hook saves an ics file once an org-buffer is saved\n(defun my-icalendar-agenda-export()\n    (if (string= (file-name-extension (buffer-file-name)) \"org\")\n              (org-icalendar-combine-agenda-files)) \n      )\n(add-hook 'after-save-hook 'my-icalendar-agenda-export)\n\n\n\nIn your dropbox folder on the web, click on share and \"get link\".\nThis links looks like this: https://www.dropbox.com/s/randomletters/org.ics\nReplace www with dl such that it looks like this: https://dl.dropbox.com/s/randomletters/org.ics\n\n\n\nThis is unfortunately not as straight-forward as I would like to belive. Google calendar allows to \"Add by url\" (in the left panel, under \"Other calendars\"). However, with dropbox-links this fails with some warning about \"robots.txt\".\nThere is the following workaround (thanks to this discussion). You have to create a pipe, then abbreviate the url with tinyurl and finally import the resulting link into google-calendar (works fine this time).\n\n\n\ne.g., mobile phone, mac, thunderbird (lightning),..."
  },
  {
    "objectID": "posts/inkscape_import_latex_pdf.html",
    "href": "posts/inkscape_import_latex_pdf.html",
    "title": "Import PDF in Inkscape without replacing fonts",
    "section": "",
    "text": "Import PDF in Inkscape without replacing fonts\nInkscape replaces text in PDF with bad fonts (especially when the PDF is generated with latex). I use Inkscape to rearrange some stuff or do some fine-tuning of a pstricks-images. Sometimes I just want to reset the bounding box (crop the PDF) without losing the PDF format. The solution is hidden in this blog post:\n\nRun the PS file filename.ps through gs to replace the fonts with outlines like so\n\ngs -sDEVICE=pswrite -dNOCACHE -sOutputFile=- -q -dbatch -dNOPAUSE -dQUIET filename.ps -c quit | ps2pdf - \"`echo filename.ps | cut -f1 -d'.'`\"-nofont.pdf\n\nto generate filename-nofont.pdf.\nThis file can be imported and used in inkscape (but you can't re-type the text, obviously)."
  },
  {
    "objectID": "posts/stuff_to_know_foreigners.html",
    "href": "posts/stuff_to_know_foreigners.html",
    "title": "Stuff you need to know as new (foreign) employee at UiT",
    "section": "",
    "text": "Even more than a year after moving to the best city on earth, Tromsø, and beginning to work at the best university in the country, the University of Tromsø, I stumble across new stuff that I wish I would have known earlier. Some of it is probably most interesting to foreigners, but most of it might be of interest to you, the Norwegian, as well :-)\nI will use this blog post to keep an updated list of these things (so check back for new entries):\n\n\nThis used to be a big problem for me. What the hell does MH AUDS6 mean? There used to be no reference whatsoever on the net what this means. Slowly, with time, I found out that MH is the health-faculty building and that AUDS6 is one of the auditoria. But at various occasions, I had a really hard time actually getting to the room indicated on my schedule.\nThis has finally changed! I just found out that there is an awesome map of all university buildings on all levels showing every room and a lot of details. And what's better: The same platform has maps for NTNU, UiB and other places.\nHere is the link: http://use.mazemap.com/\n\n\n\nYes, there is heinzelnisse for germans and dict.cc for many other languages but these dictionaries are far from good. If you are advanced, you can try the very comprehensive Bokmålsordboka (or the Nynorskordboka if you are into that sort of thing) but I often fail to understand the gist of the short explanations offered (it's norsk-norsk only). There is also something called ordnett which is available through the university's subscription but I found it frustratingly bad (unusable interface and funny search results).\nBut today, I found the perfect solution: Clue.\n\nfree for employees (instructions for access)\nmany languages\nmost complete I have seen so far\nusable from: the web, mobile phone and desktop\n\n\n\n\nThis is really cool: UiT has an unlimited-usage account with box.com. This is a service that works pretty much like Dropbox or Google Drive but has much cooler collaboration/sharing/download options and unlimited (say that again?) space! Here are UiT's instructions on how to set up your computer to use it.\nThere is excellent for data-sharing with people from all over the world, for data-archiving (unlimited, remember?!) and even for keeping your home and work computer in sync.\n\n\n\nOk, this is a bit more nerdy as I write everything that I ever write in LaTeX. I wanted to have a template for writing official letters (including university's logo etc). I ony found this clearly outdated template for MS Word and used some letters I received to come up with a nice LaTeX template myself. Here is a link to the templates (english and norwegian versions) and this is what it looks like.\nHere is how to access the git-repo for you who is from nerd-land:\ngit clone git@source.uit.no:matthias.mittner/letter_templates.git\n\n\n\nOk, another entry for nerd-people. UiT has a really good HPC cluster, stallo, that is readily accessible for UiT employees. You will have to fill out an online-application (takes 5 minutes) and you will quickly get access by the very helpful administrators. You will be using stallo's queuing system, which may need some getting used to (read the docs!).\nHere is stallo 's documentation: link."
  },
  {
    "objectID": "posts/guy_review.html",
    "href": "posts/guy_review.html",
    "title": "New paper: Toward a model-based cognitive neuroscience of mind wandering",
    "section": "",
    "text": "New paper: Toward a model-based cognitive neuroscience of mind wandering\n\n\n\n\n\nI am happy to announce that our review paper on a model-based cognitive neuroscience approach to the study of mind-wandering has been accepted for publication in Neuroscience, (Hawkins et al., accepted). This paper summarizes many of the theoretical ideas that we have been discussing in our group for quite some time and also reviews and integrates the research done so far. Actually, the number of research papers actually using cognitive process models to investigate mind wandering has been quite rare (e.g., Bastian & Sackgur, 2013) and our own paper, (Mittner et al., 2014) but there are a few instances where we can reinterpret the results in terms of mind wandering. I am really excited about the regression-based approach that is outlined in the paper and we are working on implementing this kind of model at the moment. Expect more in that direction soon!\n\nReferences\n\nHawkins G. E., Mittner, M., Boekel, W., Heathcote, A. and Forstmann, B.U. (accepted). Toward a model-based cognitive neuroscience of mind wandering. Neuroscience. link\nBastian M and Sackur J (2013) Mind wandering at the fingertips: automatic parsing of subjective states based on response time variability. Front. Psychol. 4:573. doi: 10.3389/fpsyg.2013.00573\nMittner, M., Boekel, W., Tucker, A. M., Turner, B.M., Heathcote, A. and Forstmann, B.U. (2014). When the brain takes a break: A model-based analysis of mind wandering. Journal of Neuroscience, 34(49):16286-95."
  },
  {
    "objectID": "posts/jon_comment_axelrod.html",
    "href": "posts/jon_comment_axelrod.html",
    "title": "Commentary on our mind-wandering paper",
    "section": "",
    "text": "Commentary on our mind-wandering paper\n\n\n\n\n\nVadim Axelrod and Andrei R. Teodorescu wrote an interesting commentary on our recent mind-wandering paper that was published in The Journal of Neuroscience last year. I am happy that they are supportive of our approach and glad about the constructive comments that they give in their paper.\nI am particularly happy that someone has gone to the difficulty of summarizing and contextualizing our paper because I think that it is actually quite hard to read. I think it is a strength of our paper that it combines complex from different modalities (fMRI, rsfMRI, pupil diameter, behaviour) with sophisticated analysis techniques (MVPA techniques, cognitive modeling, Bayesian stats). However, this particular feature makes the paper very hard to read, I think. I tried my best to prepare the paper in a readable way but found it quite challenging to present the paper in a both comprehensive and easy-to-read format. Axelrod & Teodorescu's comment solves this problem for me because there is now an easy-to-read summary out there that is independent of our main paper and therefore does not have the obligation to comprehensively detail the used methodology.\nI wish there was a commentary on all empirical papers...\nThere is one more comment I'd like to make regarding the commentary. In their comment, the authors focus on different model parameters that might be beneficial to study in the mind-wandering context. I do agree and I think the MW field should really dig deeper into the cognitive modeling research that has been done in other fields. For our own data however, it was not possible to focus on the parameters suggested by Axelrod and Teodorescu. As I commented in an e-mail to Vadim Axelrod:\n\nAs you note in one part of your paper, the stop-signal paradigm (at least the version we used) did not produce many behavioural errors which make it hard to constrain the more flexible models (such as those estimating between-trial variability). This was one of the reasons we pursued the relatively simple model we ended up with (and used a Bayesian hierarchical modeling approach for incorporating fits for subjects who did not have many errors).\n\n\nReferences\n\nAxelrod V and Teodorescu AR (2015) Commentary: When the brain takes a break: a model-based analysis of mind wandering. Front. Comput. Neurosci. 9:83. doi: 10.3389/fncom.2015.00083\nMittner, M., Boekel, W., Tucker, A. M., Turner, B.M., Heathcote, A. and Forstmann, B.U. (2014). When the brain takes a break: A model-based analysis of mind wandering. Journal of Neuroscience, 34(49):16286-95."
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html",
    "title": "Resources for learning (and teaching) statistics",
    "section": "",
    "text": "This is an on-going collection of resources for learning and teaching statistical concepts.\nThis post was last updated on 2022-12-20."
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#statisticial-practices",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#statisticial-practices",
    "title": "Resources for learning (and teaching) statistics",
    "section": "Statisticial practices",
    "text": "Statisticial practices\n\np-hacker app\n\nNed Bicare writes: “I developed an online app that allows to practice creative data analysis and how to polish your p-values. It’s primarily aimed at young researchers who do not have our level of expertise yet, but I guess even old hands might learn one or two new tricks! It’s called “The p-hacker” (please note that ‘hacker’ is meant in a very positive way here. You should think of the cool hackers who fight for world peace). You can use the app in teaching, or to practice p-hacking yourself.”"
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#visualisations",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#visualisations",
    "title": "Resources for learning (and teaching) statistics",
    "section": "Visualisations",
    "text": "Visualisations\n\nGuess the correlation\n\nNice retro-look game for learning to guess the strength of the correlation in a cloud of points.\n\nSeeing Theory\n\na visual introduction to probability and statistics. Many good visualisations.\n\nR<-Psychologist\n\nsome cool shiny-based visualisations of important concepts.\nExamples:\n\nPower and NHST\nConfidence Intervals\nCorrelation\nCohen’s d\n\n\nSpurious correlations\n\nunending list of spurious correlations\n\nLinear regression demo\n\nnice and clean (but french) demo for linear regression\n\nSampling Distribution Demo\n\nnice and interactive demonstration of sampling distributions"
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#cool-datasets",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#cool-datasets",
    "title": "Resources for learning (and teaching) statistics",
    "section": "Cool datasets",
    "text": "Cool datasets\n\nHuman Penguin Project\n\nthis is a cool many-labs dataset that has many interesting variables and can potentially be used for regression and ANOVA-based exercises\n\nOpen Stats Lab\n\nnice collection of data-sets from real psychological studies that come with detailed assignments to reproduce the analyses from the accompanying papers"
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#lecturestutorials",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#lecturestutorials",
    "title": "Resources for learning (and teaching) statistics",
    "section": "Lectures/tutorials",
    "text": "Lectures/tutorials\n\nModern Regression by Cosma Shalizi\n\nreally good but pretty mathematical introduction to regression. Check out the lecture scripts (e.g., this one)"
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#fun-stuff",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#fun-stuff",
    "title": "Resources for learning (and teaching) statistics",
    "section": "Fun stuff",
    "text": "Fun stuff\n\nWhy not to trust statistics\n\nnice demonstration how measures of central tendency/variance can be misleading; includes bad drawings!"
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#blogs",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#blogs",
    "title": "Resources for learning (and teaching) statistics",
    "section": "Blogs",
    "text": "Blogs\n\nAndrew Gelman\nRichard Morey (and BayesFactor and @Medium)\nFrank Harrell\nRasmus Bååth"
  },
  {
    "objectID": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#tweets",
    "href": "posts/2017-11-22-resources-for-learning-and-teaching-statistics.html#tweets",
    "title": "Resources for learning (and teaching) statistics",
    "section": "Tweets",
    "text": "Tweets\nThis is a collection of statistics-related tweets that I would like to keep…"
  },
  {
    "objectID": "posts/dic_jags.html",
    "href": "posts/dic_jags.html",
    "title": "Get Deviance Information Criterion (DIC) when sampling in JAGS",
    "section": "",
    "text": "Get Deviance Information Criterion (DIC) when sampling in JAGS\nI use the rjags package from R to sample with JAGS. When sampling with the coda.samples() function, there is no way of getting the DIC from these samples (you need to use another run with dic.samples(). When using jags.samples(), it is possible to include deviance and pD in the variable-names array\n\nvarnames=c('pD', 'deviance', 'var1', ...)\nout <- jags.samples(model, varnames, n.iter, thin, type = \"trace\")\n\nbut the resulting structure is awkward to handle and even more awkward to transfer to the nice coda-structure that allows to use all the nice plotting/diagnostic functions.\nFor complex models, the resulting overhead can be severe and I therefore set up a coda.samples.dic() function that returns both the coda-structure for the mcmc samples and the dic (the solution is inspired by a discussion in the JAGS forum):\n\ncoda.samples.dic <- function (model, variable.names = NULL, n.iter, thin = 1, ...) \n{\n      load.module('dic') # necessary for pD and deviance monitor\n\n      start <- model$iter() + thin\n      varnames=c(variable.names, c('deviance', 'pD'))\n      out <- jags.samples(model, varnames, n.iter, thin, \n           type = \"trace\", ...)\n      deviance <- out$deviance\n      pD <- out$pD\n      out$deviance <- NULL\n      out$pD <- NULL    \n      ans <- vector(\"list\", nchain(model))\n      for (ch in 1:nchain(model)) {\n           ans.ch <- vector(\"list\", length(out))\n           vnames.ch <- NULL\n           for (i in seq(along = out)) {\n                varname <- names(out)[[i]]\n                d <- dim(out[[i]])\n                if (length(d) < 3) {\n                      stop(\"Invalid dimensions for sampled output\")\n                }\n                vardim <- d[1:(length(d) - 2)]\n                nvar <- prod(vardim)\n                niter <- d[length(d) - 1]\n                nchain <- d[length(d)]\n                values <- as.vector(out[[i]])\n                var.i <- matrix(NA, nrow = niter, ncol = nvar)\n                for (j in 1:nvar) {\n                      var.i[, j] <- values[j + (0:(niter - 1)) * nvar + \n                        (ch - 1) * niter * nvar]\n                }\n                vnames.ch <- c(vnames.ch, coda.names(varname, vardim))\n                ans.ch[[i]] <- var.i\n           }\n           ans.ch <- do.call(\"cbind\", ans.ch)\n           colnames(ans.ch) <- vnames.ch\n           ans[[ch]] <- mcmc(ans.ch, start = start, thin = thin)\n      }\n\n      dic <- list(deviance = mean(as.vector(deviance)), penalty = mean(as.vector(pD)), type = 'pD')\n      class(dic) <- \"dic\"\n      return(list(samples=mcmc.list(ans), dic=dic))\n}\n\nThis code is just a copy of coda.samples() source code.\nChanges: include \"deviance\" and \"pD\" in sampling:\n\nvarnames=c(variable.names, c('deviance', 'pD'))\nout <- jags.samples(model, varnames, n.iter, thin, \n     type = \"trace\", ...)\ndeviance <- out$deviance\npD <- out$pD\nout$deviance <- NULL\nout$pD <- NULL    \n\nand return as dic structure\n\ndic <- list(deviance = mean(as.vector(deviance)), penalty = mean(as.vector(pD)), type = 'pD')\nclass(dic) <- \"dic\"\nreturn(list(samples=mcmc.list(ans), dic=dic))\n\nYou need to run the sampler with more than one chain, though."
  },
  {
    "objectID": "posts/conda.html",
    "href": "posts/conda.html",
    "title": "Handling R packages/dependencies with conda.",
    "section": "",
    "text": "Every now and then, I stumble across the case that I want to return to some analysis I have been doing some time in the past only to find out that recent developments and package updates totally break my scripts. In the python world, I have been relying on virtualenv for quite some time and it works very well: On the computer I do the work, I create a virtualenv which I enable for development. My backups do not contain this but simply the project files (where I document in the README which virtualenv is needed). When I need to run the scripts on a different computer, I just use pip freeze > requirements.txt from within the virtualenv and restore it by pip install -r requirements.txt. This has worked so far and I have not been bitten by any incompatibilities.\nRecently, I have been relying more and more on R and wanted a similar solution for that (actually, R packages change so fast, that it's almost suicide not to do it). I found that packrat is supposed to be the tool for the job. So I tried it and it has resulted in nothing but frustration:\n\nIt's NOT possible to have different versions of the R-interpreter\nThe complete library of packages is inevitably stored in the project-directory and automatically loaded once an R-interpreter is started in this directory (so I need to sync hundreds of MBs for that, especially if I run it on different operating systems).\nThe system failed my completely when I was trying to sync between our computing server and my desktop. Both systems run linux and so packrat on the cluster was just trying to reuse the compiled binaries but failed miserably because a differeng glibc version had been used for compiling the cluster R and the packages on my desktop.\n\nAnyway, I quickly dropped this solution as impractical for my use-cases. That's when I stumbled across conda which appears to offer all I need: Language-agnostic virtual environments. The article over here convinced me even more because you just have to build a file environment.yml that can be used by anyone to directly create your environment in one go (using builds that, possibly, you have to store on your own namespace on the anaconda server). I am not yet sure that conda is going to be the right tool for me but I thought I'd log some of the ups and owns while working with it for future reference.\nSo here is a log on how to do different things:\n\n\nDownload and install miniconda which is a bare-bones version of conda.\n\nwget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh\nbash Miniconda-latest-Linux-x86_64.sh\nconda update conda\nconda install anaconda-client anaconda-build conda-build\n\nAdd anaconda channel for R\n\nconda config --add channels r\n\nInstall R and, if wanted, the \"essential\" packages\n\nconda install r r-essentials\n\nCreate an account at anaconda (you will need it to make packages that are not in conda's default R-repo).\nThis will create a \"namespace\" for your chosen username which you can use to capture packages with individual versions etc.\n\n\n\nCreate a new \"virtualenv\" and switch to it\n\nconda create --name testenv r\nsource activate testenv\n\nYou can install any R-packages that are already in the repo <https://anaconda.org/R> (they are prefixed by r-), e.g.,\n\nconda install r-dplyr\n\nIf you come across a package that is not on anaconda's servers, you can easily build it yourself (if it is on CRAN; have not tried with github-packages). Sometimes, it depends on other packages that are not there, so you will need to build them, too. Here is an example for building rstan:\n\nconda skeleton cran rstan\nconda build r-rstan\n\nresults in a failure, because it depends on StanHeaders and inline. So you will need to to\n\nconda skeleton cran stanheaders\nconda skeleton cran inline\nconda build r-rstan\n\nThis will detect that you got the recipes in your local tree and it will also tell you what you have to do to upload the new packages to your own channel at anaconda (my channel is https://anaconda.org/mittner)\n\n# If you want to upload this package to anaconda.org later, type:\n#\n# $ anaconda upload /home/mittner/local/miniconda/conda-bld/linux-64/r-inline-0.3.14-r3.2.1_0.tar.bz2\n#\n# To have conda build upload to anaconda.org automatically, use\n# $ conda config --set anaconda_upload yes\n\nSo just follow that (and I also recommend setting it to \"always upload\" because, lets be honest, we tend to forget these things):\n\nconda config --set anaconda_upload yes\n\n\n\n\nNow, all you need to do to reproduce your environment on another computer (or in the future) is the equivalent to pip freeze{.bash, eval=F .sourceCode} which is\n\nconda env export --name testenv -f environment.yml\n\nIf you compiled and uploaded some packages yourself in your private channel, you might have to add the channel to the environment.yml file. The file looks like this:\nname: testenv\ndependencies:\n- cairo=1.12.18=4\n- fontconfig=2.11.1=4\n... (more packages here)\nSo you have to insert (mittner is my channel):\nname: testenv\nchannels:\n- r\n- mittner\ndependencies:\n- cairo=1.12.18=4\n- fontconfig=2.11.1=4\n... (more packages here)\nIf you put this file into your project-directory, then the user (or you) just has to run\n\nconda env create\n\nand will end up with hopefully the exact same environment you created.\n\n\n\nHere are some problems I have stumbled over so far.\nWhen using install.packages(), I get a Tcl-related error:\n\n> install.packages(\"arm\")\n\n--- Please select a CRAN mirror for use in this session ---\nError in download.file(url, destfile = f, quiet = TRUE) : \n   unsupported URL scheme\nError: .onLoad failed in loadNamespace() for 'tcltk', details:\n   call: fun(libname, pkgname)\n   error: Can't find a usable init.tcl in the following directories: \n      /opt/anaconda1anaconda2anaconda3/lib/tcl8.5 ./lib/tcl8.5 ./lib/tcl8.5 ./library ./library ./tcl8.5.18/library ./tcl8.5.18/library\nThis probably means that Tcl wasn't installed properly.\nAnd, for that matter, I don't know how conda's R-packages behave together with install.packages() (at least, they are not included in the environment.yml file, I guess?).\n\n\n\nSo far, conda looks like a promising tool, but I will have to see how it behaves in practice (I will revisit this post later to include new info)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthias Mittner’s homepage",
    "section": "",
    "text": "I am a professor at the University of Tromsø. My main interests within the field of cognitive neuroscience are mind wandering, non-invasive brain stimulation, decision making, pupillometry and selective attention. I am also interested in the development of methodological approaches in my field and enjoy computational methods, cognitive modeling, machine-learning and Bayesian statistics.\n\nmy CV\nGoogle Scholar\nLab website"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Mehmetoglu, M. & Mittner, M. (in press). Applied Statistics using R. SAGE. link\n\n\n\n\n\n\nMehmetoglu, M. & Mittner, M. (2020). Innføring i R for statistiske dataanalyser. Universitetsforlaget. link"
  },
  {
    "objectID": "publications.html#preprints",
    "href": "publications.html#preprints",
    "title": "Publications",
    "section": "Preprints",
    "text": "Preprints\nGo here for a list of all my preprints\n\n[58]  Sedlinská, T., Bolte, L., Melsæter, E., Mittner, M. & Csifcsák, G. (2022-12-16). Transcranial direct-current stimulation enhances Pavlovian tendencies during intermittent loss of control. https://psyarxiv.com/9v5sy/ status: submitted \n\n\n[57]  Groot, J., Miletic, S., Isherwood, S., Tse, D., Habli, S., Håberg, A., Forstmann, B., Bazin, P. & Mittner, M. (2022-11-03). Echoes from intrinsic connectivity networks in the subcortex. https://psyarxiv.com/xr25q/ status: submitted \n\n\n[56]  Kreis, I., Zhang, L., Mittner, M., Syla, L., Lamm, C. & Pfuhl, G. (2020-07-24). Aberrant uncertainty processing is linked to psychotic-like experiences, autistic traits and reflected in pupil dilation during probabilistic learning. https://osf.io/nc2rx/ status: unknown"
  },
  {
    "objectID": "publications.html#papers",
    "href": "publications.html#papers",
    "title": "Publications",
    "section": "Papers",
    "text": "Papers\n\n\n2023\n\n\n[55]  Nawani, H., Mittner, M. & Csifcsák, G. (2023). Modulation of Mind Wandering Using Transcranial Direct Current Stimulation: A Meta-Analysis Based on Electric Field Modeling. NeuroImage. , pp. 120051 doi:10.1016/j.neuroimage.2023.120051 \n\n\njournal\n\n\npdf\n\n\npreprint\n\n\nsupplement\n\n\n\n\n\n\n\n\n\n2022\n\n\n[54]  Alexandersen, A., Csifcsák, G., Groot, J. & Mittner, M. (2022). The Effect of Transcranial Direct Current Stimulation on the Interplay between Executive Control, Behavioral Variability and Mind Wandering: A Registered Report. Neuroimage: Reports. 2:3, pp. 100109 doi:10.1016/j.ynirp.2022.100109 \n\n\njournal\n\n\npdf\n\n\npreprint\n\n\nsupplement\n\n\n\n\n\n\n\n[53]  Groot, J., Csifcsák, G., Wientjes, S., Forstmann, B. & Mittner, M. (2022). Catching Wandering Minds with Tapping Fingers: Neural and Behavioral Insights into Task-unrelated Cognition. Cerebral Cortex. , pp. bhab494 doi:10.1093/cercor/bhab494 \n\n\njournal\n\n\npdf\n\n\npreprint\n\n\nsupplement\n\n\n\n\n\n\n\n[52]  Hawkins, G., Mittner, M., Forstmann, B. & Heathcote, A. (2022). Self-Reported Mind Wandering Reflects Executive Control and Selective Attention. Psychonomic Bulletin & Review. doi:10.3758/s13423-022-02110-3 \n\n\njournal\n\n\npdf\n\n\npreprint\n\n\nsupplement\n\n\n\n\n\n\n\n[51]  Kam, J., Mittner, M. & Knight, R. (2022). Mind-Wandering: Mechanistic Insights from Lesion, tDCS, and iEEG. Trends in Cognitive Sciences. 0:0 doi:10.1016/j.tics.2021.12.005 \n\n\njournal\n\n\n\n\n\n\n\n\n\n2021\n\n\n[50]  Csifcsák, G., Bjørkøy, J., Kuyateh, S., Reithe, H. & Mittner, M. (2021). Transcranial Direct Current Stimulation above the Medial Prefrontal Cortex Facilitates Decision-Making Following Periods of Low Outcome Controllability. eNeuro. doi:10.1523/ENEURO.0041-21.2021 \n\n\njournal\n\n\npdf\n\n\nsupplement\n\n\npreprint\n\n\n\n\n\n\n\n[49]  Groot, J., Boayue, N., Csifcsák, G., Boekel, W., Huster, R., Forstmann, B. & Mittner, M. (2021). Probing the Neural Signature of Mind Wandering with Simultaneous fMRI-EEG and Pupillometry. NeuroImage. 224, pp. 117412 doi:10.1016/j.neuroimage.2020.117412 \n\n\njournal\n\n\npdf\n\n\npreprint\n\n\nsupplement\n\n\n\n\n\n\n\n[48]  Kreis, I., Biegler, R., Tjelmeland, H., Mittner, M., Reitan, S. & Pfuhl, G. (2021). Overestimation of Volatility in Schizophrenia and Autism? A Comparative Study Using a Probabilistic Reasoning Task. PLOS ONE. 16:1, pp. e0244975 doi:10.1371/journal.pone.0244975 \n\n\njournal\n\n\n\n\n\n\n\n[47]  Rasmussen, I., Boayue, N., Mittner, M., Bystad, M., Grønli, O., Vangberg, T., Csifcsák, G. & Aslaksen, P. (2021). High-Definition Transcranial Direct Current Stimulation Improves Delayed Memory in Alzheimer’s Disease Patients: A Pilot Study Using Computational Modeling to Optimize Electrode Position. Journal of Alzheimer’s Disease. 83:2 doi:10.3233/JAD-210378 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n[46]  Turi, Z., Lenz, M., Paulus, W., Mittner, M. & Vlachos, A. (2021). Selecting Stimulation Intensity in Repetitive Transcranial Magnetic Stimulation Studies: A Systematic Review between 1991 and 2020. European Journal of Neuroscience. n/a:n/a doi:10.1111/ejn.15195 \n\n\njournal\n\n\npdf\n\n\npreprint\n\n\ngithub\n\n\n\n\n\n\n\n[45]  Zmeykina, E., Mittner, M., Paulus, W. & Turi, Z. (2021). Short-Lived Alpha Power Suppression Induced by Low-intensity Arrhythmic rTMS. Neuroscience. 466, pp. 1–9 doi:10.1016/j.neuroscience.2021.04.027 \n\n\njournal\n\n\npdf\n\n\npreprint\n\n\ngithub\n\n\n\n\n\n\n\n\n\n2020\n\n\n[44]  Boayue, N., Csifcsák, G., Kreis, I., Schmidt, C., Finn, I., Vollsund, A. & Mittner, M. (2020). The Interplay between Executive Control, Behavioral Variability and Mind Wandering: Insights from a High-Definition Transcranial Direct-Current Stimulation Study. European Journal of Neuroscience. n/a:n/a doi:10.1111/ejn.15049 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n[43]  Mittner, M. (2020). Pypillometry: A Python Package for Pupillometric Analyses. Journal of Open Source Software. 5:51, pp. 2348 doi:10.21105/joss.02348 \n\n\njournal\n\n\ngithub\n\n\n\n\n\n\n\n[42]  Turi, Z., Mittner, M., Lehr, A., Bürger, H., Antal, A. & Paulus, W. (2020). Theta-Gamma Cross-Frequency Transcranial Alternating Current Stimulation over the Trough Impairs Cognitive Control. eNeuro. doi:10.1523/ENEURO.0126-20.2020 \n\n\njournal\n\n\ngithub\n\n\npdf\n\n\npreprint\n\n\nsupplement\n\n\n\n\n\n\n\n[41]  Zmeykina, E., Mittner, M., Paulus, W. & Turi, Z. (2020). Weak rTMS-induced Electric Fields Produce Neural Entrainment in Humans. Scientific Reports. 10:1, pp. 11994 doi:10.1038/s41598-020-68687-8 \n\n\njournal\n\n\npdf\n\n\ngithub\n\n\npreprint\n\n\nsupplement\n\n\n\n\n\n\n\n\n\n2019\n\n\n[40]  Boayue, N., Csifcsák, G., Aslaksen, P., Turi, Z., Antal, A., Groot, J., Hawkins, G., Forstmann, B., Opitz, A., Thielscher, A. & Mittner, M. (2019). Increasing Propensity to Mind-Wander by Transcranial Direct Current Stimulation? A Registered Report. The European Journal of Neuroscience. doi:10.1111/ejn.14347 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n[39]  Csifcsák, G., Boayue, N., Aslaksen, P., Turi, Z., Antal, A., Groot, J., Hawkins, G., Forstmann, B., Opitz, A., Thielscher, A. & Mittner, M. (2019). Commentary: “Transcranial Stimulation of the Frontal Lobes Increases Propensity of Mind-Wandering without Changing Meta-Awareness”. Frontiers in Psychology. 10 doi:10.3389/fpsyg.2019.00130 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n[38]  Csifcsák, G., Melsæter, E. & Mittner, M. (2019). Intermittent Absence of Control during Reinforcement Learning Interferes with Pavlovian Bias in Action Selection. Journal of Cognitive Neuroscience. , pp. 1–18 doi:10.1162/jocn_a_01515 \n\n\njournal\n\n\npreprint\n\n\n\n\n\n\n\n[37]  Hawkins, G., Mittner, M., Forstmann, B. & Heathcote, A. (2019). Modeling Distracted Performance. Cognitive Psychology. 112, pp. 48–80 doi:10.1016/j.cogpsych.2019.05.002 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n[36]  Hetland, A., Kjelstrup, E., Mittner, M. & Vitterso, J. (2019). The Thrill of Speedy Descents: A Pilot Study on Differences in Facially Expressed Online Emotions and Retrospective Measures of Emotions During a Downhill Mountain-Bike Descent. Frontiers in Psychology. 10 doi:10.3389/fpsyg.2019.00566 \n\n\njournal\n\n\n\n\n\n\n\n[35]  Turi, Z., Csifcsák, G., Boayue, N., Aslaksen, P., Antal, A., Paulus, W., Groot, J., Hawkins, G., Forstmann, B., Opitz, A., Thielscher, A. & Mittner, M. (2019). Blinding Is Compromised for Transcranial Direct Current Stimulation at 1 mA for 20 Min in Young Healthy Adults. European Journal of Neuroscience. 0:0 doi:10.1111/ejn.14403 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n\n\n2018\n\n\n[34]  Boayue, N., Csifcsák, G., Puonti, O., Thielscher, A. & Mittner, M. (2018). Head Models of Healthy and Depressed Adults for~Simulating the Electric Fields of Non-Invasive Electric Brain Stimulation. F1000Research. 7, pp. 704 doi:10.12688/f1000research.15125.2 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n[33]  Csifcsák, G., Boayue, N., Puonti, O., Thielscher, A. & Mittner, M. (2018). Effects of Transcranial Direct Current Stimulation for Treating Depression: A Modeling Study. Journal of Affective Disorders. 234, pp. 164–173 doi:10.1016/j.jad.2018.02.077 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n[32]  Hetland, A., Vittersø, J., Wie, S., Kjelstrup, E., Mittner, M. & Dahl, T. (2018). Skiing and Thinking About It: Moment-to-Moment and Retrospective Analysis of Emotions in an Extreme Sport. Frontiers in Psychology. 9:971 doi:10.3389/fpsyg.2018.00971 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n[31]  Turi, Z., Schäfer, S., Antal, A., Paulus, W. & Mittner, M. (2018). Data from ‘Placebo Enhances Reward Learning in Healthy Individuals’. Journal of Open Psychology Data. 6:1, pp. 2 doi:10.5334/jopd.34 \n\n\njournal\n\n\npdf\n\n\ngithub\n\n\n\n\n\n\n\n[30]  Turi, Z., Bjørkedal, E., Gunkel, L., Antal, A., Paulus, W. & Mittner, M. (2018). Evidence for Cognitive Placebo and Nocebo Effects in Healthy Individuals. Scientific Reports. 8:1, pp. 17443 doi:10.1038/s41598-018-35124-w \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\ngithub\n\n\n\n\n\n\n\n\n\n2017\n\n\n[29]  Csifcsák, G. & Mittner, M. (2017). Linking Brain Networks and Behavioral Variability to Different Types of Mind-Wandering. Proceedings of the National Academy of Sciences of the United States of America. 114:30 doi:10.1073/pnas.1705108114 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n[28]  Hawkins, G., Mittner, M., Forstmann, B. & Heathcote, A. (2017). On the Efficiency of Neurally-Informed Cognitive Models to Identify Latent Cognitive States. Journal of Mathematical Psychology. 76, pp. 142–155 doi:10.1016/j.jmp.2016.06.007 \n\n\njournal\n\n\npdf\n\n\nsupplement\n\n\n\n\n\n\n\n[27]  Turi, Z., Mittner, M., Paulus, W. & Antal, A. (2017). Placebo Intervention Enhances Reward Learning in Healthy Individuals. Scientific Reports. 7, pp. 41028 doi:10.1038/srep41028 \n\n\njournal\n\n\npdf\n\n\ngithub\n\n\n\n\n\n\n\n\n\n2016\n\n\n[26]  Mittner, M., Hawkins, G., Boekel, W. & Forstmann, B. (2016). A Neural Model of Mind Wandering. Trends in Cognitive Sciences. 20:8, pp. 570–578 doi:10.1016/j.tics.2016.06.004 \n\n\njournal\n\n\npdf\n\n\ncorrection\n\n\n\n\n\n\n\n[25]  Rodríguez-Aranda, C., Mittner, M. & Vasylenko, O. (2016). Association Between Executive Functions, Working Memory, and Manual Dexterity in Young and Healthy Older Adults: An Exploratory Study. Perceptual and Motor Skills. 122:1, pp. 165–192 doi:10.1177/0031512516628370 \n\n\njournal\n\n\npdf\n\n\nwebsite\n\n\n\n\n\n\n\n\n\n2015\n\n\n[24]  Hawkins, G., Mittner, M., Boekel, W., Heathcote, A. & Forstmann, B. (2015). Toward a Model-Based Cognitive Neuroscience of Mind Wandering. Neuroscience. 310, pp. 290–305 doi:10.1016/j.neuroscience.2015.09.053 \n\n\njournal\n\n\npdf\n\n\nwebsite\n\n\n\n\n\n\n\n[23]  Høifødt, R., Mittner, M., Lillevoll, K., Katla, S., Kolstrup, N., Eisemann, M., Friborg, O. & Waterloo, K. (2015). Predictors of Response to Web-Based Cognitive Behavioral Therapy With High-Intensity Face-to-Face Therapist Guidance for Depression: A Bayesian Analysis. Journal of Medical Internet Research. 17:9, pp. e197 doi:10.2196/jmir.4351 \n\n\njournal\n\n\npdf\n\n\nwebsite\n\n\n\n\n\n\n\n[22]  Turi, Z., Mittner, M., Opitz, A., Popkes, M., Paulus, W. & Antal, A. (2015). Transcranial Direct Current Stimulation over the Left Prefrontal Cortex Increases Randomness of Choice in Instrumental Learning. Cortex. 63, pp. 145–154 doi:10.1016/j.cortex.2014.08.026 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n\n\n2014\n\n\n[21]  Mittner, M., Behrendt, J., Menge, U., Titz, C. & Hasselhorn, M. (2014). Response-Retrieval in Identity Negative Priming Is Modulated by Temporal Discriminability. Frontiers in Psychology. 5, pp. 621 doi:10.3389/fpsyg.2014.00621 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n[20]  Mittner, M., Boekel, W., Tucker, A., Turner, B., Heathcote, A. & Forstmann, B. (2014). When the Brain Takes a Break: A Model-Based Analysis of Mind Wandering. Journal of Neuroscience. 34:49, pp. 16286–16295 doi:10.1523/JNEUROSCI.2062-14.2014 \n\n\njournal\n\n\npdf\n\n\ncorrection\n\n\n\n\n\n\n\n\n\n2013\n\n\n[19]  Ihrke, M., Behrendt, J., Schrobsdorff, H., Visser, I. & Hasselhorn, M. (2013). Negative Priming Persists in the Absence of Response-Retrieval. Experimental Psychology. 60:1, pp. 12–21 doi:10.1027/1618-3169/a000169 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n[18]  Mittner, M. (2013). Functional Integration of Large-Scale Brain Networks. Journal of Neuroscience. 33:48, pp. 18710–18711 doi:10.1523/JNEUROSCI.4084-13.2013 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n\n\n2012\n\n\n[17]  Schrobsdorff, H., Ihrke, M., Behrendt, J., Herrmann, J. & Hasselhorn, M. (2012). Identity Negative Priming: A Phenomenon of Perception, Recognition or Selection?. PloS One. 7:3, pp. e32946 doi:10.1371/journal.pone.0032946 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n[16]  Schrobsdorff, H., Ihrke, M., Behrendt, J., Hasselhorn, M. & Herrmann, J. (2012). Inhibition in the Dynamics of Selective Attention: An Integrative Model for Negative Priming. Frontiers in Psychology. 3, pp. 491 doi:10.3389/fpsyg.2012.00491 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n\n\n2011\n\n\n[15]  Ihrke, M. & Behrendt, J. (2011). Automatic Generation of Randomized Trial Sequences for Priming Experiments. Frontiers in Psychology. 2, pp. 225 doi:10.3389/fpsyg.2011.00225 \n\n\njournal\n\n\npdf\n\n\nwebsite\n\n\n\n\n\n\n\n[14]  Ihrke, M., Schrobsdorff, H. & Herrmann, J. (2011). Recurrence-Based Estimation of Time-Distortion Functions for ERP Waveform Reconstruction. International Journal of Neural Systems. 21:1, pp. 65–78 doi:10.1142/S0129065711002651 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n[13]  Ihrke, M., Behrendt, J., Schrobsdorff, H., Herrmann, J. & Hasselhorn, M. (2011). Response-Retrieval and Negative Priming: Encoding- and Retrieval-Specific Effects.. Experimental Psychology. 58:2, pp. 154–161 doi:10.1027/1618-3169/a000081 \n\n\njournal\n\n\npreprint\n\n\npdf\n\n\n\n\n\n\n\n[12]  Ihrke, M. & Brennen, T. (2011). Sharing One Biographical Detail Elicits Priming between Famous Names: Empirical and Computational Approaches. Frontiers in Psychology. 2:75 doi:10.3389/fpsyg.2011.00075 \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n\n\n2010\n\n\n[11]  Behrendt, J., Gibbons, H., Schrobsdorff, H., Ihrke, M., Herrmann, J. & Hasselhorn, M. (2010). Event-Related Brain Potential Correlates of Identity Negative Priming from Overlapping Pictures. Psychophysiology. 47:5, pp. 921–930 doi:10.1111/j.1469-8986.2010.00989.x \n\n\njournal\n\n\npdf\n\n\n\n\n\n\n\n\n\n2009\n\n\n[10]  Ritschel, T., Ihrke, M., Frisvad, J., Coppens, J., Myszkowski, K. & Seidel, H. (2009). Temporal Glare: Real-Time Dynamic Simulation of the Scattering in the Human Eye. Computer Graphics Forum. 28:2, pp. 183–192 doi:10.1111/j.1467-8659.2009.01357.x \n\n\njournal\n\n\npdf\n\n\nwebsite\n\n\ncorrection\n\n\n\n\n\n\n\n\n\n2008\n\n\n[9]  Ritschel, T., Smith, K., Ihrke, M., Grosch, T., Myszkowski, K. & Seidel, H. (2008). 3D Unsharp Masking for Scene Coherent Enhancement. ACM Transactions on Graphics. 27:3, pp. 90:1–90:8 doi:10.1145/1399504.1360689 \n\n\njournal\n\n\npdf\n\n\nwebsite\n\n\n\n\n\n\n\n\n\n2007\n\n\n[8]  Schrobsdorff, H., Ihrke, M., Kabisch, B., Behrendt, J., Hasselhorn, M. & Herrmann, J. (2007). A Computational Approach to Negative Priming. Connection Science. 19:3, pp. 203–221 doi:10.1080/09540090701507823 \n\n\njournal\n\n\npdf"
  },
  {
    "objectID": "publications.html#book-chapters",
    "href": "publications.html#book-chapters",
    "title": "Publications",
    "section": "Book Chapters",
    "text": "Book Chapters\n\n[7]  Csifcsák, G., Forstmann, B. & Mittner, M. (2021). Transcranial stimulation and decision-making. In Wassermann, E., Peterchev, A., Ziemann, U., Lisanby, S., Siebner, H. & Walsh, V. (Eds.), The Oxford Handbook of Transcranial Stimulation. doi: \n\n\n\n\n[6]  Ihrke, M., Schrobsdorff, H. & Herrmann, J. (2009). Denoising and Averaging Techniques for Electrophysiological Data. In Velazquez, J. & Wennberg, R. (Eds.), Coordinated Activity in the Brain: Measurements and Relevance to Brain Function and Behavior. Springer New York doi:10.1007/978-0-387-93797-7_9"
  },
  {
    "objectID": "publications.html#peer-reviewed-conference-proceedings",
    "href": "publications.html#peer-reviewed-conference-proceedings",
    "title": "Publications",
    "section": "Peer-Reviewed Conference Proceedings",
    "text": "Peer-Reviewed Conference Proceedings\n\n[5]  Schrobsdorff, H., Ihrke, M. & Herrmann, J. (2013). Modeling Structure and Dynamics of Selective Attention. Advances in Intelligent Systems and Computing, Biologically Inspired Cognitive Architectures 2012, pp. 287–295. doi:10.1007/978-3-642-34274-5_50 \n\n\n\n\n[4]  Ihrke, M., Ritschel, T., Smith, K., Grosch, T., Myszkowski, K. & Seidel, H. (2009). A perceptual evaluation of 3D unsharp masking. **, Human Vision and Electronic Imaging XIV, pp. 72400R. doi:10.1117/12.809026 \n\n\n\n\n[3]  Ihrke, M., Schrobsdorff, H. & Herrmann, J. (2009). Recurrence-Based Synchronization of Single Trials for EEG-Data Analysis. Lecture Notes in Computer Science, Intelligent Data Engineering and Automated Learning - IDEAL 2009, pp. 118–125. doi:10.1007/978-3-642-04394-9_15 \n\n\n\n\n[2]  Ihrke, M., Schrobsdorff, H. & Herrmann, J. (2008). Compensation for Speed-of-Processing Effects in EEG-Data Analysis. Lecture Notes in Computer Science, Intelligent Data Engineering and Automated Learning – IDEAL 2008, pp. 354–361. doi:10.1007/978-3-540-88906-9_45 \n\n\n\n\n[1]  Yoshida, A., Ihrke, M., Mantiuk, R. & Seidel, H. (2008). Brightness of the Glare Illusion. APGV ’08, Proceedings of the 5th Symposium on Applied Perception in Graphics and Visualization, pp. 83–90. doi:10.1145/1394281.1394297"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "My GitHub profile"
  },
  {
    "objectID": "software.html#python",
    "href": "software.html#python",
    "title": "Software",
    "section": "Python",
    "text": "Python\n\n\nPypillometry https://github.io/ihrke/pypillometry \n\nThe pypillometry package implements functions for the analysis of pupillometric data. Features include preprocessing, blink handling, event-related pupil-dilation, plotting and signal modeling. Homepage | GitHub | Paper\n\n\n\n\n\nPyrace https://github.com/ihrke/pyrace \n\nRace-models in Python. Currently focus on Stop-Signal-Tasks but supports generic accumulators and experimental designs.\n\n\n\n\n\nLibEEGTools https://github.com/ihrke/libeegtools \n\nA C-library for processing EEG-data with minimal requirements. Implements filtering (bandpass, wavelets, …), dynamic time warping, clustering, time-frequency analysis, recurrence plots etc. Comes with a basic Python interface. Documentation | GitHub\n\n\n\n\n\nNipype-connect-str https://github.com/ihrke/nipype_connect_str\n\ntiny grammar for NiPype to allow for a nicer connection string of nodes in workflow"
  },
  {
    "objectID": "software.html#web-apps",
    "href": "software.html#web-apps",
    "title": "Software",
    "section": "Web-apps",
    "text": "Web-apps\n\n\nBalancinator https://github.com/ihrke/balancinator \n\nThe Balancinator is a free and open-source software to visualize distribution of men and women within a specific unit. The Balancinator allows anyone to build generic plots by inserting simple excel sheets instead of writing programming code. WebApp | GitHub\n\n\n\n\n\nBalancinator-HE https://github.com/ihrke/norgeibalanse \n\nThe Balancinator|HE is a free and open-source dashboard to visualize distribution of men and women within higher education in Norway. WebApp | GitHub"
  },
  {
    "objectID": "software.html#jspsych",
    "href": "software.html#jspsych",
    "title": "Software",
    "section": "JsPsych",
    "text": "JsPsych\n\n\njspych-plugins-mm https://github.com/ihrke/jspsych-plugins-mm \n\na couple of plugins for jsPsych I have made for our experiments. Demo | GitHub\n\n\n\n\n\npavlovian-gonogo https://github.com/ihrke/pavlovian_gonogo \n\nimplements the Pavlovian Go/NoGo task (paper)"
  },
  {
    "objectID": "software.html#r-packages",
    "href": "software.html#r-packages",
    "title": "Software",
    "section": "R-packages",
    "text": "R-packages\nThese are companion packages for my books, see my publications.\n\n\nAstatur: https://ihrke.github.io/astatur/\n\ncompanion package for: Mehmetoglu, M. & Mittner, M. (in press). Applied Statistics using R. SAGE. link\n\n\n\n\n\nRnorsk: https://ihrke.github.io/rnorsk/ \n\nMehmetoglu, M. & Mittner, M. (2020). Innføring i R for statistiske dataanalyser. Universitetsforlaget. link"
  },
  {
    "objectID": "software.html#markdown",
    "href": "software.html#markdown",
    "title": "Software",
    "section": "Markdown",
    "text": "Markdown\nI rely on pandoc and a set of templates for writing everything from papers, letters and lectures.\n\n\nmarkdown-paper https://github.com/ihrke/markdown-paper \n\na setup for writing scientific papers in markdown; supports different styles for various scientific journals (I add new ones when I submit a paper to a new journal), references through bibtex\n\n\n\n\n\nmarkdown-talk https://github.com/ihrke/markdown-talk \n\nmy setup for producing beamer-based slides for lectures and talks (PDF)\n\n\n\n\n\nmarkdown-letter https://github.com/ihrke/markdown-letter \n\nwriting latex-based letters with markdown; includes a template for my UiT’s letter head"
  },
  {
    "objectID": "pubs/preprints.html",
    "href": "pubs/preprints.html",
    "title": "Preprints",
    "section": "",
    "text": "This is a list of all preprints published by my group, so far. The preprints for which the status is “published” all have a corresponding publication on my publication list."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Resources for learning (and teaching) statistics\n\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2017\n\n\nMatthias Mittner\n\n\n\n\n\n\n  \n\n\n\n\nAssociation Between Executive Functions, Working Memory, and Manual Dexterity\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 29, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStuff you need to know as new (foreign) employee at UiT\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling R packages/dependencies with conda.\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2015\n\n\n\n\n\n\n  \n\n\n\n\nNew paper: Toward a model-based cognitive neuroscience of mind wandering\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2015\n\n\n\n\n\n\n  \n\n\n\n\nPublishing Bayesian analyses in a medical journal\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2015\n\n\n\n\n\n\n  \n\n\n\n\nCommentary on our mind-wandering paper\n\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2015\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFixing numerical underflow in WAIC calculation in Stan\n\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2015\n\n\n\n\n\n\n  \n\n\n\n\nMulti-Language IPython (Jupyter) setup\n\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2015\n\n\n\n\n\n\n  \n\n\n\n\n(Dis-)Assembling of Clavinova CLP-970\n\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGet Deviance Information Criterion (DIC) when sampling in JAGS\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccess IPython Notebook through SSH to avoid firewall\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHighlight changes with latexdiff\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2014\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetup for sharing org-mode agenda with google calendar\n\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2014\n\n\n\n\n\n\n  \n\n\n\n\nImport PDF in Inkscape without replacing fonts\n\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2013\n\n\n\n\n\n\nNo matching items"
  }
]